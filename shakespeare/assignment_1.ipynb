{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogWig1KdCop3"
      },
      "source": [
        "# Assignment 1: Building a Shakespearean Text Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3amZL81jC0ZI",
        "outputId": "42dfeb72-5f22-4cc0-c502-b075631e0b6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     /Users/ricardomendezcavalieri/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package brown to\n",
            "[nltk_data]     /Users/ricardomendezcavalieri/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfdvA59TCop4"
      },
      "source": [
        "##  Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds88tGR6Cop5",
        "outputId": "5f4da56b-3f94-4969-e6bb-ba6c60326f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'tragedie', 'of', 'julius', 'caesar', 'by', 'william', 'shakespeare', '1599', 'actus']\n"
          ]
        }
      ],
      "source": [
        "# Load All Shakespeare's Works\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from collections import defaultdict\n",
        "\n",
        "caesar = gutenberg.words('shakespeare-caesar.txt')\n",
        "hamlet = gutenberg.words('shakespeare-hamlet.txt')\n",
        "macbeth = gutenberg.words('shakespeare-macbeth.txt')\n",
        "\n",
        "# Lowercase all words, remove punctuation\n",
        "\n",
        "caesar = [word.lower() for word in caesar if word.isalnum()]\n",
        "hamlet = [word.lower() for word in hamlet if word.isalnum()]\n",
        "macbeth = [word.lower() for word in macbeth if word.isalnum()]\n",
        "\n",
        "# Tokenize the words\n",
        "\n",
        "caesar = nltk.Text(caesar)\n",
        "hamlet = nltk.Text(hamlet)\n",
        "macbeth = nltk.Text(macbeth)\n",
        "\n",
        "print(caesar.tokens[:10])\n",
        "\n",
        "#Â Bigram list\n",
        "\n",
        "caesar_bigrams = list(nltk.bigrams(caesar))\n",
        "hamlet_bigrams = list(nltk.bigrams(hamlet))\n",
        "macbeth_bigrams = list(nltk.bigrams(macbeth))\n",
        "\n",
        "from_bigram_to_next_token_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "\n",
        "# Build dictionary of bigrams to next token counts\n",
        "\n",
        "for i in range(len(caesar_bigrams) - 1):\n",
        "    bigram = caesar_bigrams[i]\n",
        "    next_word = caesar_bigrams[i + 1][1]\n",
        "    from_bigram_to_next_token_counts[bigram][next_word] += 1\n",
        "\n",
        "for i in range(len(hamlet_bigrams) - 1):\n",
        "    bigram = hamlet_bigrams[i]\n",
        "    next_word = hamlet_bigrams[i + 1][1]\n",
        "    from_bigram_to_next_token_counts[bigram][next_word] += 1\n",
        "\n",
        "for i in range(len(macbeth_bigrams) - 1):\n",
        "    bigram = macbeth_bigrams[i]\n",
        "    next_word = macbeth_bigrams[i + 1][1]\n",
        "    from_bigram_to_next_token_counts[bigram][next_word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPelWsm7Cop6"
      },
      "source": [
        "## Probability Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqe_5f4ACop6",
        "outputId": "95cd17b1-d14a-476d-8f8f-1b13bf470176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'float'>, {'of': 1.0})\n"
          ]
        }
      ],
      "source": [
        "# Calculate the probabilitiy of the next word given a bigram\n",
        "\n",
        "# Given a bigram, the probability of the next word is the frequency of the next word\n",
        "# over the total frequency of all next words for a given bigram\n",
        "\n",
        "from_bigram_to_next_token_prob = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "for bigram, next_word_counts in from_bigram_to_next_token_counts.items():\n",
        "    total = sum(next_word_counts.values())\n",
        "    for next_word, count in next_word_counts.items():\n",
        "        from_bigram_to_next_token_prob[bigram][next_word] = count / total\n",
        "\n",
        "print(from_bigram_to_next_token_prob[('the', 'tragedie')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knrMz9asCop6"
      },
      "source": [
        "## Sampling Next Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xHtfeqZoCop6",
        "outputId": "3e57fb0b-b070-4f0d-a8ab-e51655c314ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'fell'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Using np.random.choice, this function samples the next token given the list\n",
        "# of possible next words (keys) and their probabilities (values)\n",
        "\n",
        "def sample_next_token(ngram, prob_dict):\n",
        "    if ngram in prob_dict:  # Avoid KeyError\n",
        "        return str(np.random.choice(list(prob_dict[ngram].keys()), p=list(prob_dict[ngram].values())))\n",
        "    return None  # Fallback case\n",
        "\n",
        "sample_next_token(('the', 'people'), from_bigram_to_next_token_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63D-C3amCop7"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teiY4w0ICop7",
        "outputId": "33c80ae6-c143-4f60-bd97-576d1a1f5e20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['be',\n",
              " 'or',\n",
              " 'we',\n",
              " 'will',\n",
              " 'fetters',\n",
              " 'put',\n",
              " 'vpon',\n",
              " 'this',\n",
              " 'bloodie',\n",
              " 'question',\n",
              " 'you',\n",
              " 'seeme',\n",
              " 'to',\n",
              " 'feare',\n",
              " 'things',\n",
              " 'that',\n",
              " 'are',\n",
              " 'married',\n",
              " 'already',\n",
              " 'all',\n",
              " 'but',\n",
              " 'metellus',\n",
              " 'cymber',\n",
              " 'brut',\n",
              " 'they',\n",
              " 'are',\n",
              " 'vanished',\n",
              " 'calp',\n",
              " 'caesar',\n",
              " 'i',\n",
              " 'neuer',\n",
              " 'gaue',\n",
              " 'you',\n",
              " 'ought',\n",
              " 'of',\n",
              " 'woe',\n",
              " 'king',\n",
              " 'tis',\n",
              " 'deepely',\n",
              " 'sworne',\n",
              " 'sweet',\n",
              " 'leaue',\n",
              " 'me',\n",
              " 'friends',\n",
              " 'tis',\n",
              " 'now',\n",
              " 'strook',\n",
              " 'twelue',\n",
              " 'get',\n",
              " 'thee']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# This function is a generalizable way of building  a sentence given a starting n_gram, the length of the sentence\n",
        "# and the probability dictionary\n",
        "\n",
        "# It basically just samples the next word given the current n_gram and appends it to the sentence\n",
        "def build_sentence(start_ngram: tuple, length: int, ngram_prob: dict) -> list:\n",
        "    sentence = list(start_ngram)\n",
        "\n",
        "    for _ in range(length - len(start_ngram)):\n",
        "        current_ngram = tuple(sentence[-(len(start_ngram)):])\n",
        "\n",
        "        if current_ngram in ngram_prob:\n",
        "            next_word = sample_next_token(current_ngram, ngram_prob)  # Using the function\n",
        "            sentence.append(next_word)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "\n",
        "build_sentence(['be', 'or'], 50, from_bigram_to_next_token_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA2rF2dfCop7"
      },
      "source": [
        "## Different N-Gram Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qzSD5k_4Cop7"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# This is a function that gets all the texts, makes them lowercase and removes punctuation, then tokenizes them.\n",
        "# It then uses the ngrams function to get all the ngrams of a given length.\n",
        "\n",
        "# It the builds a dictionary of ngrams, that maps a ngram to a dictionary of next words and their counts, which is returned.\n",
        "# I also added a modern corpus to the mix, to see if the model can generate modern text in the same style as Shakespeare.\n",
        "\n",
        "# It has an optional parameter text, which allows you to pass a custom text to generate ngrams from.\n",
        "def get_n_grams(length, text=None):\n",
        "    \n",
        "      # Helper function that lowercases all words and removes punctuation\n",
        "    def clean_corpus(corp):\n",
        "        return [word.lower() for word in corp if word.isalnum()]\n",
        "    \n",
        "     # Helper function that processes the ngrams and builds the dictionary of frequencies\n",
        "    def process_ngrams(ngram_list, length):\n",
        "        for i in range(len(ngram_list) - length - 1):\n",
        "            ngram = ngram_list[i]\n",
        "            next_word = ngram_list[i+1][length-1]\n",
        "            from_ngram_to_next_token_counts[ngram][next_word] += 1\n",
        "    \n",
        "    if text is None:\n",
        "        caesar = gutenberg.words('shakespeare-caesar.txt')\n",
        "        hamlet = gutenberg.words('shakespeare-hamlet.txt')\n",
        "        macbeth = gutenberg.words('shakespeare-macbeth.txt')\n",
        "        modern_corpus = brown.words(categories='news') + brown.words(categories='fiction')\n",
        "\n",
        "        caesar, hamlet, macbeth, modern_corpus = clean_corpus(caesar), clean_corpus(hamlet), clean_corpus(macbeth), clean_corpus(modern_corpus)\n",
        "        \n",
        "        # Tokenize the words\n",
        "        caesar = nltk.Text(caesar)\n",
        "        hamlet = nltk.Text(hamlet)\n",
        "        macbeth = nltk.Text(macbeth)\n",
        "        modern = nltk.Text(modern_corpus)\n",
        "\n",
        "        caesar_ngrams = list(ngrams(caesar, length))\n",
        "        hamlet_ngrams = list(ngrams(hamlet, length))\n",
        "        macbeth_ngrams = list(ngrams(macbeth, length))\n",
        "        modern_ngrams = list(ngrams(modern, length))\n",
        "    else:\n",
        "        text = clean_corpus(text)\n",
        "        text = nltk.Text(text)\n",
        "        ngram_list = list(ngrams(text, length))\n",
        "\n",
        "    from_ngram_to_next_token_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "   \n",
        "    if text is None:\n",
        "        process_ngrams(caesar_ngrams, length)\n",
        "        process_ngrams(hamlet_ngrams, length)\n",
        "        process_ngrams(macbeth_ngrams, length)\n",
        "        process_ngrams(modern_ngrams, length)\n",
        "    else:\n",
        "        process_ngrams(ngram_list, length)\n",
        "\n",
        "    return from_ngram_to_next_token_counts\n",
        "\n",
        "# This function builds a new dictionary of probabilities given the counts of a dictionary of ngrams\n",
        "def get_n_grams_prob(ngram_counts):\n",
        "    from_bigram_to_next_token_prob = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    for ngram, next_word_counts in ngram_counts.items():\n",
        "        total = sum(next_word_counts.values())\n",
        "        for next_word, count in next_word_counts.items():\n",
        "            from_bigram_to_next_token_prob[ngram][next_word] = count / total\n",
        "\n",
        "    return from_bigram_to_next_token_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Qm68mbetCop8"
      },
      "outputs": [],
      "source": [
        "ngram_counts = get_n_grams(2)\n",
        "ngram_probs = get_n_grams_prob(ngram_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2yObrS5wCop8"
      },
      "outputs": [],
      "source": [
        "sentence = build_sentence(['you', 'are'], 50, ngram_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IjkVDEKEzDK",
        "outputId": "10e3da09-4396-42c7-db9e-4f2706673849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['you', 'are', 'not', 'going', 'to', 'try', 'to', 'exploit', 'the', 'land', 'of', 'heaven', 'haint', 'there', 'just', 'nothin', 'like', 'sweet', 'bels', 'iangled', 'out', 'of', 'character', 'but', 'he', 'parting', 'from', 'her', 'faire', 'iudgement', 'without', 'the', 'which', 'he', 'found', 'the', 'house', 'of', 'correction', 'the', 'jail', 'sentence', 'is', 'to', 'be', 'lost', 'he', 'stood', 'back', 'to']\n"
          ]
        }
      ],
      "source": [
        "print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhyovM6O9FAl"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjafF6pm8n1J",
        "outputId": "5e970231-cc89-4cb0-9cf3-1bf9b67d8e77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "..\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.002s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'a': 1, 'only': 1})\n",
            "defaultdict(<class 'int'>, {'test': 1})\n",
            "defaultdict(<class 'int'>, {'this': 1})\n",
            "defaultdict(<class 'int'>, {'is': 1})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import unittest\n",
        "from collections import defaultdict\n",
        "from nltk.util import ngrams\n",
        "\n",
        "mock_corpus = [\"this\", \"is\", \"a\", \"test\", \"this\", \"is\", \"only\", \"a\", \"test\"]\n",
        "\n",
        "# This function builds a dictionary of ngrams to next word counts.\n",
        "# I use it to test the function that builds the probabilities of the next word given a ngram.\n",
        "def manual_ngram_counts(corpus, n):\n",
        "    ngram_counts = defaultdict(lambda: defaultdict(int))\n",
        "    ngram_list = list(ngrams(corpus, n))\n",
        "    for i in range(len(ngram_list) - 1):\n",
        "        ngram = ngram_list[i]\n",
        "        next_word = ngram_list[i+1][-1]\n",
        "        ngram_counts[ngram][next_word] += 1\n",
        "    return ngram_counts\n",
        "\n",
        "class TestNGramFunctions(unittest.TestCase):\n",
        "\n",
        "    #  This function tests the get_n_grams function, and, more specifically, the type of the returned object.\n",
        "    # It first tests that the frequency dictionary is a defaultdict, then it tests that the inner dictionaries are defaultdicts,\n",
        "    # and finally it tests that the values of the inner dictionaries are integers.\n",
        "    \n",
        "    # It also tests that the function returns the correct frequency for a given ngram.\n",
        "    def test_get_n_grams(self):\n",
        "        length = 2\n",
        "        ngram_counts = get_n_grams(length, text=mock_corpus)\n",
        "\n",
        "        self.assertIsInstance(ngram_counts, defaultdict)\n",
        "        for key, value in ngram_counts.items():\n",
        "            self.assertIsInstance(value, defaultdict)\n",
        "            self.assertIsInstance(next(iter(value.values()), 0), int)\n",
        "        self.assertEqual(ngram_counts[('this', 'is')]['a'], 1)\n",
        "\n",
        "\n",
        "    # This function tests the get_n_grams_prob function, and, more specifically, the type of the returned object.\n",
        "    # It first tests that the probability dictionary is a defaultdict, then it tests that the inner dictionaries are defaultdicts,\n",
        "    # and finally it tests that the values of the inner dictionaries are floats.\n",
        "    \n",
        "    # It also tests that the function returns the correct probability for a given ngram by checking that the sum of the values\n",
        "    # of the inner dictionary is 1.\n",
        "    def test_get_n_grams_prob(self):\n",
        "        length = 2\n",
        "        ngram_counts = manual_ngram_counts(mock_corpus, length)\n",
        "        ngram_probs = get_n_grams_prob(ngram_counts)\n",
        "\n",
        "        self.assertIsInstance(ngram_probs, defaultdict)\n",
        "        for key, value in ngram_probs.items():\n",
        "            self.assertIsInstance(value, defaultdict)\n",
        "            self.assertIsInstance(next(iter(value.values()), 0.0), float)\n",
        "\n",
        "        for key, value in ngram_probs.items():\n",
        "            self.assertAlmostEqual(sum(value.values()), 1.0, places=5)\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestNGramFunctions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7p0ZbW68nUH"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIAgRJCnCop8"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
